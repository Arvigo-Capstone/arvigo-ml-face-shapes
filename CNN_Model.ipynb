{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "S3dsmhX_wDjp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l1, l2\n",
        "from PIL import Image, ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive to access dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set up directory paths\n",
        "base_dir = '/content/drive/MyDrive/Dataset Hasil Split 80-20_100x100_0'\n",
        "h5_dir = '/content/drive/MyDrive/Capstone Project ML/CNN_Model3.h5'\n",
        "train_dir = os.path.join(base_dir, 'training')\n",
        "test_dir = os.path.join(base_dir, 'validation')"
      ],
      "metadata": {
        "id": "YW7bSAEMoggZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a97a962b-975c-4f3a-c944-04209a69ed55"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define the model\n",
        "# model = Sequential([\n",
        "#     Conv2D(32, (3,3), activation='relu', input_shape=(100,100,3)),\n",
        "#     MaxPooling2D((2,2)),\n",
        "#     Conv2D(64, (3,3), activation='relu'),\n",
        "#     MaxPooling2D((2,2)),\n",
        "#     Conv2D(128, (3,3), activation='relu'),\n",
        "#     MaxPooling2D((2,2)),\n",
        "#     Conv2D(256, (3,3), activation='relu'),\n",
        "#     MaxPooling2D((2,2)),\n",
        "#     Flatten(),\n",
        "#     Dense(512, activation='relu'),\n",
        "#     Dropout(0.2),\n",
        "#     Dense(256, activation='relu'),\n",
        "#     Dropout(0.2),\n",
        "#     Dense(5, activation='softmax')\n",
        "# ])\n",
        "\n",
        "# # Compile the model\n",
        "# opt = Adam(learning_rate=0.0001)\n",
        "# model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Jsi8UoRjwdVB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load the pre-trained model from the .h5 file\n",
        "# model = tf.keras.models.load_model(h5_dir)"
      ],
      "metadata": {
        "id": "n0R7z5Gj6avQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up data generators for training and validation\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                   rotation_range=20,\n",
        "                                   brightness_range=(0.8,1.2),\n",
        "                                   horizontal_flip=True)\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                 rotation_range=20,\n",
        "                                 brightness_range=(0.8,1.2),\n",
        "                                 horizontal_flip=True)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(train_dir,\n",
        "                                                    target_size=(100, 100),\n",
        "                                                    batch_size=16,\n",
        "                                                    class_mode='categorical')\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(test_dir,\n",
        "                                                target_size=(100, 100),\n",
        "                                                batch_size=16,\n",
        "                                                class_mode='categorical')"
      ],
      "metadata": {
        "id": "pQo5BNVxwMtv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "e4ac5fd9-a0d8-4542-9490-6f10c7ee20cc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-6204e877eb30>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m                                  horizontal_flip=True)\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m train_generator = train_datagen.flow_from_directory(train_dir,\n\u001b[0m\u001b[1;32m     13\u001b[0m                                                     \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                                                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m   1646\u001b[0m                 \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0my\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m         \"\"\"\n\u001b[0;32m-> 1648\u001b[0;31m         return DirectoryIterator(\n\u001b[0m\u001b[1;32m   1649\u001b[0m             \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1650\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Dataset Hasil Split 80-20_100x100_0/training'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up checkpoint callback\n",
        "checkpoint = ModelCheckpoint(\"CNN_Model3.h5\", \n",
        "                             monitor='val_accuracy', \n",
        "                             verbose=1, \n",
        "                             save_best_only=True, \n",
        "                             mode='max')"
      ],
      "metadata": {
        "id": "29YcYJURwTRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(train_generator,\n",
        "                    epochs=40,\n",
        "                    validation_data=val_generator,\n",
        "                    callbacks=[checkpoint])"
      ],
      "metadata": {
        "id": "NkGQFEMLwZki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-evaluate the model\n",
        "loss, acc = model.evaluate(val_generator, verbose=2)\n",
        "print(\"Accuracy: {:5.2f}%\".format(100 * acc))"
      ],
      "metadata": {
        "id": "G_PQZ44SwWwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))"
      ],
      "metadata": {
        "id": "8dI0QtcBxDvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a figure with two subplots\n",
        "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
        "\n",
        "# Plotting accuracy\n",
        "ax[0].plot(epochs, acc, 'r', linewidth=2, label='Training Accuracy')\n",
        "ax[0].plot(epochs, val_acc, 'b', linewidth=2, label='Validation Accuracy')\n",
        "ax[0].set_title('Training and Validation Accuracy', fontsize=16)\n",
        "ax[0].set_xlabel('Epoch', fontsize=14)\n",
        "ax[0].set_ylabel('Accuracy', fontsize=14)\n",
        "ax[0].legend(loc='lower right', fontsize=12)\n",
        "ax[0].grid()\n",
        "ax[0].tick_params(axis='both', labelsize=12)\n",
        "\n",
        "# Plotting loss\n",
        "ax[1].plot(epochs, loss, 'r', linewidth=2, label='Training Loss')\n",
        "ax[1].plot(epochs, val_loss, 'b', linewidth=2, label='Validation Loss')\n",
        "ax[1].set_title('Training and Validation Loss', fontsize=16)\n",
        "ax[1].set_xlabel('Epoch', fontsize=14)\n",
        "ax[1].set_ylabel('Loss', fontsize=14)\n",
        "ax[1].legend(loc='upper right', fontsize=12)\n",
        "ax[1].grid()\n",
        "ax[1].tick_params(axis='both', labelsize=12)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ast6SDlwxD_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the predicted labels for all images in the cross validation set\n",
        "pred = model.predict(val_generator)\n",
        "predicted_labels = np.argmax(pred, axis=1)\n",
        "\n",
        "# Get the true labels for all images in the cross validation set\n",
        "true_labels = val_generator.classes\n",
        "\n",
        "# Get the filenames of the images in the cross validation set\n",
        "filenames = val_generator.filenames\n",
        "\n",
        "# Get the indices of the misclassified images\n",
        "misclassified_indices = np.where(predicted_labels != true_labels)[0]\n",
        "\n",
        "# Get the misclassified images and their true and predicted labels\n",
        "misclassified_images = []\n",
        "misclassified_true_labels = []\n",
        "misclassified_predicted_labels = []\n",
        "\n",
        "for i in misclassified_indices:\n",
        "    misclassified_images.append(filenames[i])\n",
        "    misclassified_true_labels.append(true_labels[i])\n",
        "    misclassified_predicted_labels.append(predicted_labels[i])\n",
        "\n",
        "# Print the misclassified images and their true and predicted labels\n",
        "print(\"Misclassified Images:\")\n",
        "for i in range(len(misclassified_images)):\n",
        "    print(\"Image:\", misclassified_images[i], \"True Label:\", misclassified_true_labels[i], \"Predicted Label:\", misclassified_predicted_labels[i])"
      ],
      "metadata": {
        "id": "to4huGqpxIBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display some of the misclassified images\n",
        "num_images_to_display = 36\n",
        "fig, axes = plt.subplots(6, 6, figsize=(12, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "# Map class indices to class names\n",
        "class_names = {v: k for k, v in val_generator.class_indices.items()}\n",
        "\n",
        "for i in range(num_images_to_display):\n",
        "    img = Image.open(os.path.join(test_dir, misclassified_images[i]))\n",
        "    true_label = class_names[misclassified_true_labels[i]]\n",
        "    predicted_label = class_names[misclassified_predicted_labels[i]]\n",
        "    axes[i].imshow(img)\n",
        "    axes[i].set_title(\"True: {}\\nPredicted: {}\".format(true_label, predicted_label), fontsize=10)\n",
        "    axes[i].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B_aIcuQ9xKNn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}